{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21bcd0b5-a3b4-429c-85e0-0b117249f727",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbf0218f-7af9-4c10-b601-5fef6c890cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "import hw3_main\n",
    "from helper import *\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a21378b",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "495682d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feature_vector_challenge(df):\n",
    "    static_variables = config['static']\n",
    "    timeseries_variables = config['timeseries']\n",
    "    feature_dict = {}\n",
    "\n",
    "    for var in static_variables:\n",
    "        val = df[df[\"Variable\"] == var][\"Value\"].values[0]\n",
    "        feature_dict[var] = val\n",
    "\n",
    "        if val < 0:\n",
    "            feature_dict[var] = np.nan\n",
    "        else:\n",
    "            feature_dict[var] = val\n",
    "\n",
    "    # Time-varying variables\n",
    "    for var in timeseries_variables:\n",
    "        if (df['Variable'] == var).sum() == 0:\n",
    "            fval = np.nan\n",
    "            sval = np.nan\n",
    "            norm_sd = np.nan\n",
    "        else:\n",
    "            fval = (df[(df[\"Variable\"] == var) & (df[\"Time\"].str[:2].astype(int) < 24)][\"Value\"]).mean()\n",
    "            sval = (df[(df[\"Variable\"] == var) & (df[\"Time\"].str[:2].astype(int) >= 24)][\"Value\"]).mean()\n",
    "\n",
    "            mean = (df[(df[\"Variable\"] == var)][\"Value\"]).mean()\n",
    "            if mean == 0:\n",
    "                norm_sd = np.nan\n",
    "            else:\n",
    "                sd = (df[(df[\"Variable\"] == var)][\"Value\"]).std()\n",
    "                norm_sd = sd / mean\n",
    "\n",
    "        feature_dict[f\"f24_mean_{var}\"] = fval\n",
    "        feature_dict[f\"s24_mean_{var}\"] = sval\n",
    "        feature_dict[f\"norm_sd_{var}\"] = norm_sd\n",
    "\n",
    "    return feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "795cf6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_feature_matrix_challenge(X):\n",
    "    return hw3_main.normalize_feature_matrix(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0d14217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_values_challenge(X):\n",
    "    for col in range(X.shape[1]):\n",
    "        median = np.nanmedian(X[:, col])\n",
    "        X[:, col] = np.where(np.isnan(X[:, col]), median, X[:, col])\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05179568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_split(X: np.ndarray[float], y: np.ndarray[int]):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, stratify=y, random_state=69)\n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbb9637e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_challenge(X_challenge, y_challenge, X_heldout, feature_names, metric=\"auroc\"):\n",
    "    print(\"================= Part 3 ===================\")\n",
    "    print(\"Part 3: Challenge\")\n",
    "\n",
    "    X_train, X_val, y_train, y_val = get_train_val_split(X_challenge, y_challenge)\n",
    "    assert X_train.shape[0] == y_train.size and X_val.shape[0] == y_val.size\n",
    "\n",
    "    alpha_range = np.logspace(-4, 4, 9)\n",
    "    penalties = [\"l1\", \"l2\", \"elasticnet\"]\n",
    "    scores = []\n",
    "\n",
    "    for alpha in alpha_range:\n",
    "        for penalty in penalties:\n",
    "            clf = SGDClassifier(loss=\"modified_huber\", alpha=alpha, penalty=penalty)\n",
    "            clf.fit(X_train, y_train)\n",
    "\n",
    "            score = hw3_main.cv_performance(clf, X_train, y_train, 10, metric)\n",
    "            print(\"alpha: {:.6f} \\t penalty: {:10s} \\t score: {:.4f}\".format(alpha, penalty, score))\n",
    "            scores.append((alpha, penalty, score))\n",
    "\n",
    "    best = sorted(scores, key=lambda x: x[2], reverse=True)[0]\n",
    "    clf = SGDClassifier(loss=\"modified_huber\", alpha=best[0], penalty=best[1])\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    test_perf = hw3_main.performance(clf, X_val, y_val, metric)\n",
    "    print(\"alpha = \" + str(best[0]) + \"\\npenalty = \" + str(best[1]) +\n",
    "          \"\\nTest Performance on metric \" + metric + \": %.4f\" % test_perf)\n",
    "\n",
    "    metric_list = [\"accuracy\", \"precision\", \"sensitivity\", \"specificity\", \"f1_score\", \"auroc\", \"auprc\"]\n",
    "\n",
    "    for metric in metric_list:\n",
    "        test_perf = hw3_main.performance(clf, X_val, y_val, metric)\n",
    "        print(\"Validation Performance on metric \" + metric + \": %.4f\" % test_perf)\n",
    "\n",
    "    y_score = clf.predict_proba(X_heldout)[:, 1]\n",
    "    y_label = clf.predict(X_heldout)\n",
    "    make_challenge_submission(y_label, y_score)\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b951aba9",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c484752a",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67524c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files from disk: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12000/12000 [00:24<00:00, 487.44it/s]\n",
      "Generating feature vectors: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12000/12000 [06:42<00:00, 29.80it/s]\n"
     ]
    }
   ],
   "source": [
    "X_challenge, y_challenge, X_heldout, feature_names = get_challenge_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88af4b10",
   "metadata": {},
   "source": [
    "## Run tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3601170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================= Part 3 ===================\n",
      "Part 3: Challenge\n",
      "alpha: 0.000100 \t penalty: l1         \t score: 0.8275\n",
      "alpha: 0.000100 \t penalty: l2         \t score: 0.8310\n",
      "alpha: 0.000100 \t penalty: elasticnet \t score: 0.8291\n",
      "alpha: 0.001000 \t penalty: l1         \t score: 0.8325\n",
      "alpha: 0.001000 \t penalty: l2         \t score: 0.8337\n",
      "alpha: 0.001000 \t penalty: elasticnet \t score: 0.8341\n",
      "alpha: 0.010000 \t penalty: l1         \t score: 0.7958\n",
      "alpha: 0.010000 \t penalty: l2         \t score: 0.8245\n",
      "alpha: 0.010000 \t penalty: elasticnet \t score: 0.8213\n",
      "alpha: 0.100000 \t penalty: l1         \t score: 0.6991\n",
      "alpha: 0.100000 \t penalty: l2         \t score: 0.8000\n",
      "alpha: 0.100000 \t penalty: elasticnet \t score: 0.7740\n",
      "alpha: 1.000000 \t penalty: l1         \t score: 0.5000\n",
      "alpha: 1.000000 \t penalty: l2         \t score: 0.7504\n",
      "alpha: 1.000000 \t penalty: elasticnet \t score: 0.5000\n",
      "alpha: 10.000000 \t penalty: l1         \t score: 0.5000\n",
      "alpha: 10.000000 \t penalty: l2         \t score: 0.6304\n",
      "alpha: 10.000000 \t penalty: elasticnet \t score: 0.5000\n",
      "alpha: 100.000000 \t penalty: l1         \t score: 0.5000\n",
      "alpha: 100.000000 \t penalty: l2         \t score: 0.7140\n",
      "alpha: 100.000000 \t penalty: elasticnet \t score: 0.5000\n",
      "alpha: 1000.000000 \t penalty: l1         \t score: 0.5000\n",
      "alpha: 1000.000000 \t penalty: l2         \t score: 0.6334\n",
      "alpha: 1000.000000 \t penalty: elasticnet \t score: 0.5000\n",
      "alpha: 10000.000000 \t penalty: l1         \t score: 0.5000\n",
      "alpha: 10000.000000 \t penalty: l2         \t score: 0.6259\n",
      "alpha: 10000.000000 \t penalty: elasticnet \t score: 0.5000\n",
      "alpha = 0.001\n",
      "penalty = elasticnet\n",
      "Test Performance on metric auroc: 0.8272\n",
      "Validation Performance on metric accuracy: 0.8545\n",
      "Validation Performance on metric precision: 0.5548\n",
      "Validation Performance on metric sensitivity: 0.2792\n",
      "Validation Performance on metric specificity: 0.9592\n",
      "Validation Performance on metric f1_score: 0.3715\n",
      "Validation Performance on metric auroc: 0.8272\n",
      "Validation Performance on metric auprc: 0.4595\n",
      "Saving challenge output...\n",
      "challenge.csv saved\n"
     ]
    }
   ],
   "source": [
    "# TODO: Question 3: Apply a classifier to heldout features, and then use\n",
    "#       generate_challenge_labels to print the predicted labels\n",
    "auroc_clf = run_challenge(X_challenge, y_challenge, X_heldout, feature_names, \"auroc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3eb2116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================= Part 3 ===================\n",
      "Part 3: Challenge\n",
      "alpha: 0.000100 \t penalty: l1         \t score: 0.2494\n",
      "alpha: 0.000100 \t penalty: l2         \t score: 0.3783\n",
      "alpha: 0.000100 \t penalty: elasticnet \t score: 0.3171\n",
      "alpha: 0.001000 \t penalty: l1         \t score: 0.2833\n",
      "alpha: 0.001000 \t penalty: l2         \t score: 0.3193\n",
      "alpha: 0.001000 \t penalty: elasticnet \t score: 0.2795\n",
      "alpha: 0.010000 \t penalty: l1         \t score: 0.0849\n",
      "alpha: 0.010000 \t penalty: l2         \t score: 0.2010\n",
      "alpha: 0.010000 \t penalty: elasticnet \t score: 0.1708\n",
      "alpha: 0.100000 \t penalty: l1         \t score: 0.0000\n",
      "alpha: 0.100000 \t penalty: l2         \t score: 0.0064\n",
      "alpha: 0.100000 \t penalty: elasticnet \t score: 0.0000\n",
      "alpha: 1.000000 \t penalty: l1         \t score: 0.0000\n",
      "alpha: 1.000000 \t penalty: l2         \t score: 0.0000\n",
      "alpha: 1.000000 \t penalty: elasticnet \t score: 0.0000\n",
      "alpha: 10.000000 \t penalty: l1         \t score: 0.0000\n",
      "alpha: 10.000000 \t penalty: l2         \t score: 0.0000\n",
      "alpha: 10.000000 \t penalty: elasticnet \t score: 0.0000\n",
      "alpha: 100.000000 \t penalty: l1         \t score: 0.0801\n",
      "alpha: 100.000000 \t penalty: l2         \t score: 0.0533\n",
      "alpha: 100.000000 \t penalty: elasticnet \t score: 0.0533\n",
      "alpha: 1000.000000 \t penalty: l1         \t score: 0.0533\n",
      "alpha: 1000.000000 \t penalty: l2         \t score: 0.0267\n",
      "alpha: 1000.000000 \t penalty: elasticnet \t score: 0.0800\n",
      "alpha: 10000.000000 \t penalty: l1         \t score: 0.0000\n",
      "alpha: 10000.000000 \t penalty: l2         \t score: 0.1070\n",
      "alpha: 10000.000000 \t penalty: elasticnet \t score: 0.0000\n",
      "alpha = 0.0001\n",
      "penalty = l2\n",
      "Test Performance on metric f1_score: 0.3388\n",
      "Validation Performance on metric accuracy: 0.8595\n",
      "Validation Performance on metric precision: 0.6154\n",
      "Validation Performance on metric sensitivity: 0.2338\n",
      "Validation Performance on metric specificity: 0.9734\n",
      "Validation Performance on metric f1_score: 0.3388\n",
      "Validation Performance on metric auroc: 0.8263\n",
      "Validation Performance on metric auprc: 0.4604\n",
      "Saving challenge output...\n",
      "challenge.csv saved\n"
     ]
    }
   ],
   "source": [
    "f1_clf = run_challenge(X_challenge, y_challenge, X_heldout, feature_names, \"f1_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7449f42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: csv file is valid.\n"
     ]
    }
   ],
   "source": [
    "test_challenge_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5d55ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(f1_clf.predict(X_challenge), y_challenge, normalize='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8008409e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.88210121, 0.11789879],\n",
       "       [0.32924962, 0.67075038]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6cb89e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS334",
   "language": "python",
   "name": "cs334"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
